{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to LLM Council","text":"<p> A framework for querying multiple Large Language Models and synthesizing their collective intelligence. </p>"},{"location":"#what-is-llm-council","title":"What is LLM Council?","text":"<p>LLM Council is an open-source framework that enables you to query multiple Large Language Models (LLMs) simultaneously and synthesize their responses into a single, well-reasoned answer.</p> <p>Instead of relying on a single AI model, LLM Council leverages the collective intelligence of multiple models through a structured deliberation process:</p> <ul> <li>Multiple Perspectives: Each council member (LLM) provides their independent answer</li> <li>Peer Review: Models evaluate and rank each other's responses</li> <li>Synthesis: A chairman model produces a final answer incorporating the best insights</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> <p> Parallel Queries</p> <p>Query multiple LLMs simultaneously for faster results</p> </li> <li> <p> Peer Review System</p> <p>Models critique and rank each other's responses</p> </li> <li> <p> Intelligent Synthesis</p> <p>Chairman model combines the best insights into one answer</p> </li> <li> <p> Multi-turn Conversations</p> <p>Continue conversations with context from previous rounds</p> </li> </ul>"},{"location":"#how-it-works","title":"How It Works","text":"<pre><code>graph LR\n    A[Your Question] --&gt; B[Council Members]\n    B --&gt; C[Model 1]\n    B --&gt; D[Model 2]\n    B --&gt; E[Model 3]\n    C --&gt; F[Peer Reviews]\n    D --&gt; F\n    E --&gt; F\n    F --&gt; G[Chairman]\n    G --&gt; H[Final Answer]</code></pre> <ol> <li>Submit a Question - Send your question to the council</li> <li>Collect Responses - Each LLM provides their independent answer</li> <li>Peer Review - Models evaluate and rank each other's responses</li> <li>Synthesis - The chairman analyzes everything and produces the final answer</li> </ol>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import requests\n\n# Create a new session\nresponse = requests.post(\"http://localhost:8000/session\", json={\n    \"question\": \"What are the best practices for API design?\"\n})\nsession_id = response.json()[\"session\"][\"id\"]\n\n# Run the full council process\nresult = requests.post(f\"http://localhost:8000/session/{session_id}/run-all\")\nfinal_answer = result.json()[\"session\"][\"rounds\"][0][\"final_synthesis\"]\n\nprint(final_answer)\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to get started? Check out the Installation Guide to set up LLM Council, or jump straight to the Quickstart if you prefer learning by doing.</p> <ul> <li> <p> Installation</p> <p>Set up LLM Council on your machine</p> </li> <li> <p> Quickstart</p> <p>Get up and running in 5 minutes</p> </li> <li> <p> Concepts</p> <p>Understand how the council works</p> </li> <li> <p> API Reference</p> <p>Explore the full API documentation</p> </li> </ul>"},{"location":"#community","title":"Community","text":"<ul> <li>GitHub: Report issues or contribute</li> <li>Discussions: Ask questions and share ideas</li> </ul> <p> LLM Council v0.0.7 </p>"},{"location":"api/models/","title":"Models API","text":"<p>Reference for the models endpoint.</p>"},{"location":"api/models/#get-models","title":"Get Models","text":"<p>Retrieve the configured council models and chairman.</p> <pre><code>GET /models\n</code></pre>"},{"location":"api/models/#response","title":"Response","text":"<pre><code>{\n  \"council_models\": [\n    {\n      \"id\": \"nvidia/nemotron-nano-9b-v2:free\",\n      \"name\": \"NVIDIA Nemotron 9B\",\n      \"provider\": \"openrouter\"\n    },\n    {\n      \"id\": \"nvidia/nemotron-nano-12b-v2-vl:free\",\n      \"name\": \"NVIDIA: Nemotron Nano 12B 2 VL\",\n      \"provider\": \"openrouter\"\n    },\n    {\n      \"id\": \"google/gemma-3-27b-it:free\",\n      \"name\": \"Gemma 3 27B\",\n      \"provider\": \"openrouter\"\n    },\n    {\n      \"id\": \"openai/gpt-oss-20b:free\",\n      \"name\": \"GPT OSS 20B\",\n      \"provider\": \"openrouter\"\n    }\n  ],\n  \"chairman_model\": {\n    \"id\": \"x-ai/grok-4.1-fast:free\",\n    \"name\": \"Grok 4.1 Fast\",\n    \"provider\": \"openrouter\"\n  }\n}\n</code></pre>"},{"location":"api/models/#response-fields","title":"Response Fields","text":""},{"location":"api/models/#council_models","title":"council_models","text":"<p>Array of council member models.</p> Field Type Description <code>id</code> string Model identifier used with the provider <code>name</code> string Human-readable display name <code>provider</code> string The LLM provider (e.g., \"openrouter\")"},{"location":"api/models/#chairman_model","title":"chairman_model","text":"<p>The model responsible for synthesis.</p> Field Type Description <code>id</code> string Model identifier <code>name</code> string Display name <code>provider</code> string The LLM provider"},{"location":"api/models/#example","title":"Example","text":""},{"location":"api/models/#python","title":"Python","text":"<pre><code>  import requests\n\n  response = requests.get(\"http://localhost:8000/models\")\n  data = response.json()\n\n  print(\"Council Members:\")\n  for model in data[\"council_models\"]:\n      print(f\"  - {model['name']} ({model['id']})\")\n\n  print(f\"\\nChairman: {data['chairman_model']['name']}\")\n</code></pre>"},{"location":"api/models/#curl","title":"cURL","text":"<pre><code>  curl http://localhost:8000/models | jq\n</code></pre>"},{"location":"api/models/#javascript","title":"JavaScript","text":"<pre><code>const response = await fetch(\"http://localhost:8000/models\");\nconst data = await response.json();\n\nconsole.log(\"Council Members:\");\ndata.council_models.forEach((model) =&gt; {\n  console.log(`  - ${model.name} (${model.id})`);\n});\n\nconsole.log(`\\nChairman: ${data.chairman_model.name}`);\n</code></pre>"},{"location":"api/models/#output","title":"Output","text":"<pre><code>Council Members:\n  - NVIDIA Nemotron 9B (nvidia/nemotron-nano-9b-v2:free)\n  - NVIDIA: Nemotron Nano 12B 2 VL (nvidia/nemotron-nano-12b-v2-vl:free)\n  - Gemma 3 27B (google/gemma-3-27b-it:free)\n  - GPT OSS 20B (openai/gpt-oss-20b:free)\n\nChairman: Grok 4.1 Fast\n</code></pre>"},{"location":"api/models/#changing-models","title":"Changing Models","text":"<p>Models are configured in <code>backend/config.py</code>. See the Configuration Guide for details.</p> <p>Runtime Changes</p> <p>Currently, model configuration requires a server restart. Dynamic model configuration is planned for a future release.</p>"},{"location":"api/overview/","title":"API Overview","text":"<p>The LLM Council REST API allows you to programmatically interact with the council.</p>"},{"location":"api/overview/#base-url","title":"Base URL","text":"<pre><code>http://localhost:8000\n</code></pre>"},{"location":"api/overview/#authentication","title":"Authentication","text":"<p>Currently, the API does not require authentication. This will be added in a future release.</p>"},{"location":"api/overview/#response-format","title":"Response Format","text":"<p>All responses are JSON formatted.</p>"},{"location":"api/overview/#success-response","title":"Success Response","text":"<pre><code>{\n  \"session\": { ... },\n  \"message\": \"Operation completed successfully\"\n}\n</code></pre>"},{"location":"api/overview/#error-response","title":"Error Response","text":"<pre><code>{\n  \"detail\": \"Error description\"\n}\n</code></pre>"},{"location":"api/overview/#http-status-codes","title":"HTTP Status Codes","text":"Code Description <code>200</code> Success <code>400</code> Bad request (invalid input) <code>404</code> Resource not found <code>500</code> Internal server error"},{"location":"api/overview/#endpoints-summary","title":"Endpoints Summary","text":""},{"location":"api/overview/#health","title":"Health","text":"Method Endpoint Description <code>GET</code> <code>/</code> Health check"},{"location":"api/overview/#sessions","title":"Sessions","text":"Method Endpoint Description <code>GET</code> <code>/sessions</code> List all sessions <code>POST</code> <code>/session</code> Create new session <code>GET</code> <code>/session/{id}</code> Get session details <code>DELETE</code> <code>/session/{id}</code> Delete session <code>POST</code> <code>/session/{id}/continue</code> Continue with follow-up <code>POST</code> <code>/session/{id}/responses</code> Collect council responses <code>POST</code> <code>/session/{id}/reviews</code> Collect peer reviews <code>POST</code> <code>/session/{id}/synthesize</code> Get final synthesis <code>POST</code> <code>/session/{id}/run-all</code> Run full process"},{"location":"api/overview/#models","title":"Models","text":"Method Endpoint Description <code>GET</code> <code>/models</code> Get configured models"},{"location":"api/overview/#quick-start-example","title":"Quick Start Example","text":"<pre><code>import requests\n\nBASE = \"http://localhost:8000\"\n\n# 1. Create a session\nresp = requests.post(f\"{BASE}/session\", json={\"question\": \"What is Python?\"})\nsession_id = resp.json()[\"session\"][\"id\"]\n\n# 2. Run the full council\nresult = requests.post(f\"{BASE}/session/{session_id}/run-all\")\n\n# 3. Get the answer\nanswer = result.json()[\"session\"][\"rounds\"][0][\"final_synthesis\"]\nprint(answer)\n</code></pre>"},{"location":"api/overview/#interactive-documentation","title":"Interactive Documentation","text":"<p>The API provides interactive documentation:</p> <ul> <li>ReDoc: http://localhost:8000/redoc</li> <li>Swagger UI: http://localhost:8000/docs</li> </ul>"},{"location":"api/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Sessions API - Detailed session endpoints</li> <li>Models API - Models endpoint reference</li> </ul>"},{"location":"api/sessions/","title":"Sessions API","text":"<p>Complete reference for session management endpoints.</p>"},{"location":"api/sessions/#list-sessions","title":"List Sessions","text":"<p>Retrieve all council sessions.</p> <pre><code>GET /sessions\n</code></pre>"},{"location":"api/sessions/#parameters","title":"Parameters","text":"Name Type Default Description <code>limit</code> integer 50 Maximum sessions to return"},{"location":"api/sessions/#response","title":"Response","text":"<pre><code>{\n  \"sessions\": [\n    {\n      \"id\": \"550e8400-e29b-41d4-a716-446655440000\",\n      \"title\": \"API Design Best Practices\",\n      \"question\": \"What are API design best practices?\",\n      \"status\": \"synthesized\",\n      \"round_count\": 2,\n      \"created_at\": \"2024-01-15T10:30:00Z\"\n    }\n  ],\n  \"count\": 1\n}\n</code></pre>"},{"location":"api/sessions/#example","title":"Example","text":""},{"location":"api/sessions/#python","title":"Python","text":"<pre><code>  import requests\n\n  response = requests.get(\"http://localhost:8000/sessions\")\n  sessions = response.json()[\"sessions\"]\n\n  for session in sessions:\n      print(f\"{session['title']} - {session['status']}\")\n</code></pre>"},{"location":"api/sessions/#curl","title":"cURL","text":"<pre><code>  curl http://localhost:8000/sessions\n</code></pre>"},{"location":"api/sessions/#create-session","title":"Create Session","text":"<p>Start a new council session with a question.</p> <pre><code>POST /session\n</code></pre>"},{"location":"api/sessions/#request-body","title":"Request Body","text":"<pre><code>{\n  \"question\": \"What are the best practices for error handling?\"\n}\n</code></pre> Field Type Required Description <code>question</code> string Yes The question to ask the council"},{"location":"api/sessions/#response_1","title":"Response","text":"<pre><code>{\n  \"session\": {\n    \"id\": \"550e8400-e29b-41d4-a716-446655440000\",\n    \"title\": \"What are the best practices for error handling?\",\n    \"rounds\": [\n      {\n        \"question\": \"What are the best practices for error handling?\",\n        \"responses\": [],\n        \"peer_reviews\": [],\n        \"final_synthesis\": null,\n        \"status\": \"pending\"\n      }\n    ],\n    \"is_deleted\": false,\n    \"deleted_at\": null\n  },\n  \"message\": \"Session created. Call /session/{id}/responses to get council responses.\"\n}\n</code></pre>"},{"location":"api/sessions/#example_1","title":"Example","text":""},{"location":"api/sessions/#python_1","title":"Python","text":"<pre><code>  import requests\n\n  response = requests.post(\n      \"http://localhost:8000/session\",\n      json={\"question\": \"What is machine learning?\"}\n  )\n\n  session_id = response.json()[\"session\"][\"id\"]\n  print(f\"Created session: {session_id}\")\n</code></pre>"},{"location":"api/sessions/#curl_1","title":"cURL","text":"<pre><code>  curl -X POST http://localhost:8000/session \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"question\": \"What is machine learning?\"}'\n</code></pre>"},{"location":"api/sessions/#get-session","title":"Get Session","text":"<p>Retrieve a session by ID.</p> <pre><code>GET /session/{session_id}\n</code></pre>"},{"location":"api/sessions/#path-parameters","title":"Path Parameters","text":"Name Type Description <code>session_id</code> string The session UUID"},{"location":"api/sessions/#response_2","title":"Response","text":"<p>Returns the full session object with all rounds, responses, and reviews.</p>"},{"location":"api/sessions/#example_2","title":"Example","text":""},{"location":"api/sessions/#python_2","title":"Python","text":"<pre><code>  import requests\n\n  session_id = \"550e8400-e29b-41d4-a716-446655440000\"\n  response = requests.get(f\"http://localhost:8000/session/{session_id}\")\n\n  session = response.json()[\"session\"]\n  print(f\"Status: {session['rounds'][-1]['status']}\")\n</code></pre>"},{"location":"api/sessions/#curl_2","title":"cURL","text":"<pre><code>  curl http://localhost:8000/session/550e8400-e29b-41d4-a716-446655440000\n</code></pre>"},{"location":"api/sessions/#delete-session","title":"Delete Session","text":"<p>Soft-delete a session.</p> <pre><code>DELETE /session/{session_id}\n</code></pre>"},{"location":"api/sessions/#response_3","title":"Response","text":"<pre><code>{\n  \"message\": \"Session deleted\"\n}\n</code></pre>"},{"location":"api/sessions/#continue-session","title":"Continue Session","text":"<p>Add a follow-up question to an existing session.</p> <pre><code>POST /session/{session_id}/continue\n</code></pre> <p>Prerequisite</p> <p>The previous round must be fully completed (status: <code>synthesized</code>) before continuing.</p>"},{"location":"api/sessions/#request-body_1","title":"Request Body","text":"<pre><code>{\n  \"question\": \"Can you provide specific examples?\"\n}\n</code></pre>"},{"location":"api/sessions/#response_4","title":"Response","text":"<p>Returns the updated session with a new pending round.</p>"},{"location":"api/sessions/#example_3","title":"Example","text":"<pre><code>import requests\n\nsession_id = \"550e8400-e29b-41d4-a716-446655440000\"\n\n# Add follow-up question\nresponse = requests.post(\n    f\"http://localhost:8000/session/{session_id}/continue\",\n    json={\"question\": \"Can you elaborate on point 2?\"}\n)\n\n# Run the council for the new round\nresult = requests.post(f\"http://localhost:8000/session/{session_id}/run-all\")\n</code></pre>"},{"location":"api/sessions/#collect-responses","title":"Collect Responses","text":"<p>Query all council members for their responses.</p> <pre><code>POST /session/{session_id}/responses\n</code></pre> <p>Step 1 of 3 in the council process.</p>"},{"location":"api/sessions/#response_5","title":"Response","text":"<p>Returns the session with updated responses in the current round.</p> <pre><code>{\n  \"session\": {\n    \"rounds\": [\n      {\n        \"status\": \"responses_complete\",\n        \"responses\": [\n          {\n            \"model_id\": \"openai/gpt-4o\",\n            \"model_name\": \"GPT-4o\",\n            \"response\": \"Here are my thoughts...\",\n            \"error\": null\n          }\n          // ... more responses\n        ]\n      }\n    ]\n  },\n  \"message\": \"All council responses collected. Call /session/{id}/reviews for peer reviews.\"\n}\n</code></pre>"},{"location":"api/sessions/#collect-reviews","title":"Collect Reviews","text":"<p>Have each council member review and rank others' responses.</p> <pre><code>POST /session/{session_id}/reviews\n</code></pre> <p>Step 2 of 3 in the council process.</p> <p>Prerequisite</p> <p>Responses must be collected first (status: <code>responses_complete</code>).</p>"},{"location":"api/sessions/#response_6","title":"Response","text":"<p>Returns the session with peer reviews added.</p> <pre><code>{\n  \"session\": {\n    \"rounds\": [\n      {\n        \"status\": \"reviews_complete\",\n        \"peer_reviews\": [\n          {\n            \"reviewer_model\": \"openai/gpt-4o\",\n            \"rankings\": [\n              {\n                \"model_id\": \"anthropic/claude-3.5-sonnet\",\n                \"rank\": 1,\n                \"score\": 8.5\n              },\n              { \"model_id\": \"google/gemini-pro-1.5\", \"rank\": 2, \"score\": 7.0 }\n            ]\n          }\n        ]\n      }\n    ]\n  },\n  \"message\": \"Peer reviews complete. Call /session/{id}/synthesize for final answer.\"\n}\n</code></pre>"},{"location":"api/sessions/#synthesize","title":"Synthesize","text":"<p>Have the chairman produce the final synthesized answer.</p> <pre><code>POST /session/{session_id}/synthesize\n</code></pre> <p>Step 3 of 3 in the council process.</p>"},{"location":"api/sessions/#response_7","title":"Response","text":"<pre><code>{\n  \"session\": {\n    \"rounds\": [\n      {\n        \"status\": \"synthesized\",\n        \"final_synthesis\": \"Based on the council's deliberation, here is a comprehensive answer...\"\n      }\n    ]\n  },\n  \"message\": \"Synthesis complete!\"\n}\n</code></pre>"},{"location":"api/sessions/#run-full-process","title":"Run Full Process","text":"<p>Execute the complete council deliberation in one call.</p> <pre><code>POST /session/{session_id}/run-all\n</code></pre> <p>Recommended</p> <p>This is the recommended endpoint for most use cases. It runs all three phases automatically.</p>"},{"location":"api/sessions/#what-it-does","title":"What It Does","text":"<ol> <li>Collects responses from all council members</li> <li>Gathers peer reviews</li> <li>Produces the final synthesis</li> </ol>"},{"location":"api/sessions/#response_8","title":"Response","text":"<p>Returns the fully completed session.</p>"},{"location":"api/sessions/#example_4","title":"Example","text":""},{"location":"api/sessions/#python_3","title":"Python","text":"<pre><code>  import requests\n\n  # Create and run in two calls\n  session = requests.post(\n      \"http://localhost:8000/session\",\n      json={\"question\": \"What is quantum computing?\"}\n  ).json()\n\n  result = requests.post(\n      f\"http://localhost:8000/session/{session['session']['id']}/run-all\"\n  ).json()\n\n  # Get the final answer\n  final_answer = result[\"session\"][\"rounds\"][0][\"final_synthesis\"]\n  print(final_answer)\n</code></pre>"},{"location":"api/sessions/#curl_3","title":"cURL","text":"<pre><code>  # Create session\n  SESSION_ID=$(curl -s -X POST http://localhost:8000/session \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"question\": \"What is quantum computing?\"}' | jq -r '.session.id')\n\n  # Run full process\n  curl -X POST \"http://localhost:8000/session/$SESSION_ID/run-all\" | jq '.session.rounds[0].final_synthesis'\n</code></pre>"},{"location":"api/sessions/#session-status-reference","title":"Session Status Reference","text":"Status Description Next Step <code>pending</code> Waiting for responses Call <code>/responses</code> <code>responses_complete</code> All responses collected Call <code>/reviews</code> <code>reviews_complete</code> Peer reviews done Call <code>/synthesize</code> <code>synthesized</code> Process complete Call <code>/continue</code> or done"},{"location":"concepts/council-process/","title":"Council Process","text":"<p>A detailed look at each phase of the council deliberation.</p>"},{"location":"concepts/council-process/#process-overview","title":"Process Overview","text":"<p>The council process consists of three distinct phases, each building on the previous one.</p> <pre><code>sequenceDiagram\n    participant U as User\n    participant API as LLM Council API\n    participant C1 as Council Member 1\n    participant C2 as Council Member 2\n    participant C3 as Council Member 3\n    participant CH as Chairman\n\n    U-&gt;&gt;API: Submit Question\n\n    par Parallel Queries\n        API-&gt;&gt;C1: Query\n        API-&gt;&gt;C2: Query\n        API-&gt;&gt;C3: Query\n    end\n\n    C1--&gt;&gt;API: Response\n    C2--&gt;&gt;API: Response\n    C3--&gt;&gt;API: Response\n\n    par Peer Reviews\n        API-&gt;&gt;C1: Review others\n        API-&gt;&gt;C2: Review others\n        API-&gt;&gt;C3: Review others\n    end\n\n    C1--&gt;&gt;API: Rankings\n    C2--&gt;&gt;API: Rankings\n    C3--&gt;&gt;API: Rankings\n\n    API-&gt;&gt;CH: All data\n    CH--&gt;&gt;API: Synthesis\n    API--&gt;&gt;U: Final Answer</code></pre>"},{"location":"concepts/council-process/#phase-1-response-collection","title":"Phase 1: Response Collection","text":""},{"location":"concepts/council-process/#what-happens","title":"What Happens","text":"<ol> <li>Your question is sent to all council members simultaneously</li> <li>Each model generates its response independently</li> <li>Responses are collected and stored</li> </ol>"},{"location":"concepts/council-process/#the-prompt","title":"The Prompt","text":"<p>Each council member receives a prompt like:</p> <pre><code>You are a member of an AI council. Please provide your best answer to the following question.\n\nQuestion: {user_question}\n\nPrevious context (if any): {previous_rounds}\n\nProvide a thorough, well-reasoned response.\n</code></pre>"},{"location":"concepts/council-process/#response-format","title":"Response Format","text":"<pre><code>{\n  \"model_id\": \"openai/gpt-4o\",\n  \"model_name\": \"GPT-4o\",\n  \"response\": \"The model's complete response...\",\n  \"error\": null\n}\n</code></pre>"},{"location":"concepts/council-process/#handling-failures","title":"Handling Failures","text":"<p>If a model fails to respond:</p> <ul> <li>The error is recorded</li> <li>Other models continue normally</li> <li>Peer review and synthesis proceed with available responses</li> </ul>"},{"location":"concepts/council-process/#phase-2-peer-review","title":"Phase 2: Peer Review","text":""},{"location":"concepts/council-process/#what-happens_1","title":"What Happens","text":"<ol> <li>Each model receives all other models' responses</li> <li>They evaluate and rank each response</li> <li>They provide reasoning for their rankings</li> </ol>"},{"location":"concepts/council-process/#the-review-prompt","title":"The Review Prompt","text":"<pre><code>You are reviewing responses from other AI models.\n\nOriginal Question: {question}\n\nResponses to review:\n1. Model A: {response_a}\n2. Model B: {response_b}\n\nPlease rank these responses and explain your reasoning.\nConsider: accuracy, completeness, clarity, and helpfulness.\n</code></pre>"},{"location":"concepts/council-process/#review-format","title":"Review Format","text":"<pre><code>{\n  \"reviewer_model\": \"anthropic/claude-3.5-sonnet\",\n  \"rankings\": [\n    {\n      \"model_id\": \"openai/gpt-4o\",\n      \"rank\": 1,\n      \"score\": 8.5,\n      \"reasoning\": \"Comprehensive and well-structured...\"\n    },\n    {\n      \"model_id\": \"google/gemini-pro-1.5\",\n      \"rank\": 2,\n      \"score\": 7.0,\n      \"reasoning\": \"Good but lacks specific examples...\"\n    }\n  ]\n}\n</code></pre>"},{"location":"concepts/council-process/#why-peer-review-matters","title":"Why Peer Review Matters","text":"<p>Quality Signal</p> <p>Peer reviews provide a quality signal that helps the chairman identify the strongest responses and most valuable insights.</p> <ul> <li>Consensus Detection: If all reviewers agree, high confidence</li> <li>Disagreement Analysis: Different opinions reveal nuance</li> <li>Error Catching: Models often catch each other's mistakes</li> </ul>"},{"location":"concepts/council-process/#phase-3-synthesis","title":"Phase 3: Synthesis","text":""},{"location":"concepts/council-process/#what-happens_2","title":"What Happens","text":"<ol> <li>The chairman receives all responses and reviews</li> <li>It analyzes the collective input</li> <li>It produces a unified, comprehensive answer</li> </ol>"},{"location":"concepts/council-process/#the-synthesis-prompt","title":"The Synthesis Prompt","text":"<pre><code>You are the chairman of an AI council. Your task is to synthesize\na final answer based on the council's deliberation.\n\nOriginal Question: {question}\n\nCouncil Responses:\n{all_responses}\n\nPeer Reviews:\n{all_reviews}\n\nBased on all perspectives and the peer review feedback, provide\na comprehensive final answer that:\n1. Incorporates the strongest points from each response\n2. Addresses any disagreements or inconsistencies\n3. Presents a clear, unified answer\n</code></pre>"},{"location":"concepts/council-process/#what-makes-good-synthesis","title":"What Makes Good Synthesis","text":"<p>The chairman should:</p> <ul> <li>Identify Common Ground: Points all models agree on</li> <li>Resolve Conflicts: Address disagreements thoughtfully</li> <li>Add Value: Not just summarize, but integrate insights</li> <li>Maintain Coherence: Produce a readable, unified response</li> </ul>"},{"location":"concepts/council-process/#status-tracking","title":"Status Tracking","text":"<p>Each round progresses through statuses:</p> Status Description <code>pending</code> Round created, waiting for responses <code>responses_complete</code> All council responses collected <code>reviews_complete</code> Peer reviews finished <code>synthesized</code> Final answer produced"},{"location":"concepts/council-process/#continuing-conversations","title":"Continuing Conversations","text":"<p>After synthesis, you can continue with a follow-up question:</p> <pre><code>flowchart LR\n    R1[Round 1] --&gt; R2[Round 2] --&gt; R3[Round 3]\n\n    subgraph R1\n        Q1[Question] --&gt; A1[Answer]\n    end\n\n    subgraph R2\n        Q2[Follow-up] --&gt; A2[Answer with context]\n    end\n\n    subgraph R3\n        Q3[More questions] --&gt; A3[Deeper insights]\n    end</code></pre> <p>The council maintains context from previous rounds, allowing for deeper exploration of topics.</p>"},{"location":"concepts/council-process/#next-steps","title":"Next Steps","text":"<ul> <li>Models - Learn about model selection</li> <li>API Reference - Explore the session endpoints</li> </ul>"},{"location":"concepts/how-it-works/","title":"How It Works","text":"<p>Understand the architecture and flow of LLM Council.</p>"},{"location":"concepts/how-it-works/#overview","title":"Overview","text":"<p>LLM Council is built on the principle that collective intelligence from multiple AI models produces better results than any single model alone.</p> <pre><code>flowchart TD\n    subgraph Input\n        Q[User Question]\n    end\n\n    subgraph Council[\"Council Members\"]\n        M1[Model 1]\n        M2[Model 2]\n        M3[Model 3]\n    end\n\n    subgraph Review[\"Peer Review\"]\n        R1[Review 1]\n        R2[Review 2]\n        R3[Review 3]\n    end\n\n    subgraph Synthesis\n        C[Chairman Model]\n    end\n\n    subgraph Output\n        A[Final Answer]\n    end\n\n    Q --&gt; M1 &amp; M2 &amp; M3\n    M1 &amp; M2 &amp; M3 --&gt; R1 &amp; R2 &amp; R3\n    R1 &amp; R2 &amp; R3 --&gt; C\n    C --&gt; A</code></pre>"},{"location":"concepts/how-it-works/#the-three-phases","title":"The Three Phases","text":""},{"location":"concepts/how-it-works/#phase-1-collect-responses","title":"Phase 1: Collect Responses","text":"<p>Each council member LLM receives the same question and generates an independent response.</p> <p>Why parallel queries?</p> <ul> <li>Models don't influence each other</li> <li>Each provides their unique perspective</li> <li>Different models have different strengths</li> </ul> <pre><code>Question: \"What are best practices for error handling?\"\n\nGPT-4o: \"Focus on specific exception types...\"\nClaude: \"Error handling should be defensive...\"\nGemini: \"Consider the error hierarchy...\"\n</code></pre>"},{"location":"concepts/how-it-works/#phase-2-peer-review","title":"Phase 2: Peer Review","text":"<p>Each model reviews and ranks the other models' responses.</p> <p>What happens during peer review:</p> <ol> <li>Each model sees all other responses</li> <li>They evaluate based on accuracy, completeness, and clarity</li> <li>They provide rankings and reasoning</li> </ol> <pre><code>Claude reviewing others:\n1. GPT-4o (8/10) - \"Comprehensive but could be more structured\"\n2. Gemini (7/10) - \"Good examples but missing edge cases\"\n</code></pre> <p>Benefits:</p> <ul> <li>Identifies the strongest responses</li> <li>Catches errors or inconsistencies</li> <li>Provides multiple quality assessments</li> </ul>"},{"location":"concepts/how-it-works/#phase-3-synthesis","title":"Phase 3: Synthesis","text":"<p>The chairman model analyzes all responses and reviews to produce the final answer.</p> <p>The chairman considers:</p> <ul> <li>All individual responses</li> <li>Peer review rankings and feedback</li> <li>Common themes across responses</li> <li>Unique insights from each model</li> </ul> <p>The result:</p> <p>A comprehensive answer that incorporates the best elements from all council members while maintaining coherence.</p>"},{"location":"concepts/how-it-works/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Frontend                             \u2502\n\u2502                    (React + Vite)                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502 HTTP/REST\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Backend API                             \u2502\n\u2502                  (FastAPI + Python)                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n\u2502  \u2502   Routers   \u2502  \u2502  Services   \u2502  \u2502   Schemas   \u2502          \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u25bc                           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      MongoDB        \u2502     \u2502     OpenRouter      \u2502\n\u2502   (Session Store)   \u2502     \u2502    (LLM Gateway)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u25bc                 \u25bc                 \u25bc\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502  GPT-4o  \u2502     \u2502  Claude  \u2502     \u2502  Gemini  \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concepts/how-it-works/#key-components","title":"Key Components","text":""},{"location":"concepts/how-it-works/#sessions","title":"Sessions","text":"<p>A session represents a conversation with the council. Sessions can contain multiple rounds of questions and answers.</p> <pre><code>{\n  \"id\": \"uuid\",\n  \"title\": \"API Design Best Practices\",\n  \"rounds\": [\n    {\n      \"question\": \"What are API design best practices?\",\n      \"responses\": [...],\n      \"peer_reviews\": [...],\n      \"final_synthesis\": \"...\",\n      \"status\": \"synthesized\"\n    }\n  ]\n}\n</code></pre>"},{"location":"concepts/how-it-works/#rounds","title":"Rounds","text":"<p>Each round is a single question-answer cycle containing:</p> <ul> <li>The question</li> <li>Individual model responses</li> <li>Peer reviews</li> <li>The synthesized final answer</li> <li>Status tracking</li> </ul>"},{"location":"concepts/how-it-works/#status-flow","title":"Status Flow","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; pending: Create Session\n    pending --&gt; responses_complete: Collect Responses\n    responses_complete --&gt; reviews_complete: Peer Review\n    reviews_complete --&gt; synthesized: Synthesis\n    synthesized --&gt; pending: Continue (new round)\n    synthesized --&gt; [*]: Done</code></pre>"},{"location":"concepts/how-it-works/#why-this-approach","title":"Why This Approach?","text":""},{"location":"concepts/how-it-works/#advantages","title":"Advantages","text":"Benefit Description Diverse Perspectives Different models have different training and biases Error Reduction Peer review catches mistakes Quality Assurance Multiple evaluations ensure accuracy Comprehensive Answers Final synthesis combines best insights Transparency You can see each model's response"},{"location":"concepts/how-it-works/#trade-offs","title":"Trade-offs","text":"Consideration Impact Cost Multiple API calls per question Latency Full process takes longer than single query Complexity More moving parts than simple queries"},{"location":"concepts/how-it-works/#next-steps","title":"Next Steps","text":"<ul> <li>Council Process Details - Deep dive into each phase</li> <li>Models - Learn about model selection</li> <li>API Reference - Explore the endpoints</li> </ul>"},{"location":"concepts/models/","title":"Models","text":"<p>Understanding and configuring the LLM models in your council.</p>"},{"location":"concepts/models/#model-roles","title":"Model Roles","text":"<p>LLM Council uses two types of model roles:</p>"},{"location":"concepts/models/#council-members","title":"Council Members","text":"<p>These are the LLMs that:</p> <ul> <li>Respond to user questions</li> <li>Review each other's responses</li> <li>Provide diverse perspectives</li> </ul> <p>Recommended: 3-5 models for good diversity without excessive cost.</p>"},{"location":"concepts/models/#chairman","title":"Chairman","text":"<p>The chairman is a single LLM responsible for:</p> <ul> <li>Analyzing all council responses</li> <li>Considering peer review feedback</li> <li>Producing the final synthesized answer</li> </ul> <p>Recommended: Use a capable model like Grok 4.1 Fast for synthesis.</p>"},{"location":"concepts/models/#choosing-models","title":"Choosing Models","text":""},{"location":"concepts/models/#diversity-is-key","title":"Diversity is Key","text":"<p>Select models with different:</p> <ul> <li>Training approaches (different companies)</li> <li>Strengths (coding, analysis, creativity)</li> <li>Perspectives (different knowledge cutoffs)</li> </ul>"},{"location":"concepts/models/#default-configuration-free-tier","title":"Default Configuration (Free Tier)","text":"<p>LLM Council uses free-tier models by default for zero-cost operation:</p> <pre><code>COUNCIL_MODELS = [\n    {\n        \"id\": \"nvidia/nemotron-nano-9b-v2:free\",\n        \"name\": \"NVIDIA Nemotron 9B\",\n        \"provider\": \"openrouter\"\n    },\n    {\n        \"id\": \"nvidia/nemotron-nano-12b-v2-vl:free\",\n        \"name\": \"NVIDIA: Nemotron Nano 12B 2 VL\",\n        \"provider\": \"openrouter\"\n    },\n    {\n        \"id\": \"google/gemma-3-27b-it:free\",\n        \"name\": \"Gemma 3 27B\",\n        \"provider\": \"openrouter\"\n    },\n    {\n        \"id\": \"openai/gpt-oss-20b:free\",\n        \"name\": \"GPT OSS 20B\",\n        \"provider\": \"openrouter\"\n    },\n]\n\nCHAIRMAN_MODEL = {\n    \"id\": \"x-ai/grok-4.1-fast:free\",\n    \"name\": \"Grok 4.1 Fast\",\n    \"provider\": \"openrouter\"\n}\n</code></pre>"},{"location":"concepts/models/#alternative-premium-models","title":"Alternative: Premium Models","text":"<p>For higher quality responses, you can use paid models:</p> <pre><code>COUNCIL_MODELS = [\n    {\"id\": \"openai/gpt-4o\", \"name\": \"GPT-4o\", \"provider\": \"openrouter\"},\n    {\"id\": \"anthropic/claude-3.5-sonnet\", \"name\": \"Claude 3.5\", \"provider\": \"openrouter\"},\n    {\"id\": \"google/gemini-pro-1.5\", \"name\": \"Gemini Pro\", \"provider\": \"openrouter\"},\n]\n\nCHAIRMAN_MODEL = {\n    \"id\": \"anthropic/claude-3.5-sonnet\",\n    \"name\": \"Claude 3.5 Sonnet (Chairman)\",\n    \"provider\": \"openrouter\"\n}\n</code></pre>"},{"location":"concepts/models/#current-models","title":"Current Models","text":""},{"location":"concepts/models/#council-members_1","title":"Council Members","text":"Model ID Provider Notes NVIDIA Nemotron 9B <code>nvidia/nemotron-nano-9b-v2:free</code> NVIDIA Fast, efficient NVIDIA Nemotron 12B VL <code>nvidia/nemotron-nano-12b-v2-vl:free</code> NVIDIA Vision-language capable Gemma 3 27B <code>google/gemma-3-27b-it:free</code> Google Strong reasoning GPT OSS 20B <code>openai/gpt-oss-20b:free</code> OpenAI Open source variant"},{"location":"concepts/models/#chairman_1","title":"Chairman","text":"Model ID Provider Notes Grok 4.1 Fast <code>x-ai/grok-4.1-fast:free</code> xAI Fast synthesis"},{"location":"concepts/models/#cost-considerations","title":"Cost Considerations","text":"<p>With free-tier models, LLM Council operates at zero cost!</p> <p>For paid models, the cost calculation is:</p> <pre><code>Cost per query \u2248\n    (Council Members \u00d7 2) + 1\n\n    - Each member responds once\n    - Each member reviews once\n    - Chairman synthesizes once\n</code></pre>"},{"location":"concepts/models/#example-cost-calculation","title":"Example Cost Calculation","text":"<p>With 4 council members + 1 chairman:</p> Phase API Calls Models Used Responses 4 Council members Peer Reviews 4 Council members Synthesis 1 Chairman Total 9 <p>Free Tier Advantage</p> <p>Using the default free-tier models means unlimited queries at no cost. Perfect for development, testing, and personal use!</p>"},{"location":"concepts/models/#viewing-current-configuration","title":"Viewing Current Configuration","text":"<p>Check your current models via the API:</p> <pre><code>curl http://localhost:8000/models | jq\n</code></pre> <p>Response:</p> <pre><code>{\n  \"council_models\": [\n    {\n      \"id\": \"nvidia/nemotron-nano-9b-v2:free\",\n      \"name\": \"NVIDIA Nemotron 9B\",\n      \"provider\": \"openrouter\"\n    },\n    {\n      \"id\": \"nvidia/nemotron-nano-12b-v2-vl:free\",\n      \"name\": \"NVIDIA: Nemotron Nano 12B 2 VL\",\n      \"provider\": \"openrouter\"\n    },\n    {\n      \"id\": \"google/gemma-3-27b-it:free\",\n      \"name\": \"Gemma 3 27B\",\n      \"provider\": \"openrouter\"\n    },\n    {\n      \"id\": \"openai/gpt-oss-20b:free\",\n      \"name\": \"GPT OSS 20B\",\n      \"provider\": \"openrouter\"\n    }\n  ],\n  \"chairman_model\": {\n    \"id\": \"x-ai/grok-4.1-fast:free\",\n    \"name\": \"Grok 4.1 Fast\",\n    \"provider\": \"openrouter\"\n  }\n}\n</code></pre>"},{"location":"concepts/models/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - How to change models</li> <li>API Reference - Models API endpoint</li> </ul>"},{"location":"examples/basic-usage/","title":"Basic Usage","text":"<p>Learn the fundamentals of using LLM Council through practical examples.</p>"},{"location":"examples/basic-usage/#simple-query","title":"Simple Query","text":"<p>The most basic usage: ask a question and get a synthesized answer.</p>"},{"location":"examples/basic-usage/#python","title":"Python","text":"<pre><code>import requests\n\nBASE_URL = \"http://localhost:8000\"\n\ndef ask_council(question: str) -&gt; str:\n    \"\"\"Ask the council a question and get the synthesized answer.\"\"\"\n    # Create session\n    response = requests.post(f\"{BASE_URL}/session\", json={\"question\": question})\n    session_id = response.json()[\"session\"][\"id\"]\n\n    # Run full council process\n    result = requests.post(f\"{BASE_URL}/session/{session_id}/run-all\")\n\n    # Extract final answer\n    return result.json()[\"session\"][\"rounds\"][0][\"final_synthesis\"]\n\n\n# Usage\nanswer = ask_council(\"What are the key principles of clean code?\")\nprint(answer)\n</code></pre>"},{"location":"examples/basic-usage/#javascriptnodejs","title":"JavaScript/Node.js","text":"<pre><code>const BASE_URL = \"http://localhost:8000\";\n\nasync function askCouncil(question) {\n  // Create session\n  const sessionRes = await fetch(`${BASE_URL}/session`, {\n    method: \"POST\",\n    headers: { \"Content-Type\": \"application/json\" },\n    body: JSON.stringify({ question }),\n  });\n  const { session } = await sessionRes.json();\n\n  // Run full council process\n  const resultRes = await fetch(`${BASE_URL}/session/${session.id}/run-all`, {\n    method: \"POST\",\n  });\n  const result = await resultRes.json();\n\n  // Extract final answer\n  return result.session.rounds[0].final_synthesis;\n}\n\n// Usage\naskCouncil(\"What are the key principles of clean code?\").then((answer) =&gt;\n  console.log(answer)\n);\n</code></pre>"},{"location":"examples/basic-usage/#accessing-individual-responses","title":"Accessing Individual Responses","text":"<p>Sometimes you want to see what each council member said, not just the synthesis.</p> <pre><code>import requests\n\nBASE_URL = \"http://localhost:8000\"\n\ndef get_all_perspectives(question: str):\n    \"\"\"Get individual responses from each council member.\"\"\"\n    # Create and run\n    response = requests.post(f\"{BASE_URL}/session\", json={\"question\": question})\n    session_id = response.json()[\"session\"][\"id\"]\n    result = requests.post(f\"{BASE_URL}/session/{session_id}/run-all\")\n\n    round_data = result.json()[\"session\"][\"rounds\"][0]\n\n    # Print each model's response\n    print(\"=\" * 60)\n    print(\"INDIVIDUAL RESPONSES\")\n    print(\"=\" * 60)\n\n    for resp in round_data[\"responses\"]:\n        print(f\"\\n{resp['model_name']}\")\n        print(\"-\" * 40)\n        print(resp[\"response\"])\n\n    # Print synthesis\n    print(\"\\n\" + \"=\" * 60)\n    print(\"SYNTHESIZED ANSWER\")\n    print(\"=\" * 60)\n    print(round_data[\"final_synthesis\"])\n\n\n# Usage\nget_all_perspectives(\"Should I use SQL or NoSQL for my next project?\")\n</code></pre>"},{"location":"examples/basic-usage/#viewing-peer-reviews","title":"Viewing Peer Reviews","text":"<p>See how models evaluated each other.</p> <pre><code>import requests\n\nBASE_URL = \"http://localhost:8000\"\n\ndef show_peer_reviews(question: str):\n    \"\"\"Display peer review rankings.\"\"\"\n    response = requests.post(f\"{BASE_URL}/session\", json={\"question\": question})\n    session_id = response.json()[\"session\"][\"id\"]\n    result = requests.post(f\"{BASE_URL}/session/{session_id}/run-all\")\n\n    round_data = result.json()[\"session\"][\"rounds\"][0]\n\n    print(\"PEER REVIEW RESULTS\")\n    print(\"=\" * 60)\n\n    for review in round_data[\"peer_reviews\"]:\n        print(f\"\\n\ud83d\udd0d Reviewer: {review['reviewer_model']}\")\n        print(\"-\" * 40)\n\n        for ranking in review[\"rankings\"]:\n            print(f\"  {ranking['rank']}. {ranking.get('model_id', 'Unknown')}\")\n            if 'score' in ranking:\n                print(f\"     Score: {ranking['score']}/10\")\n            if 'reasoning' in ranking:\n                print(f\"     Reason: {ranking['reasoning'][:100]}...\")\n\n\n# Usage\nshow_peer_reviews(\"What's the best programming language for beginners?\")\n</code></pre>"},{"location":"examples/basic-usage/#step-by-step-processing","title":"Step-by-Step Processing","text":"<p>For more control, run each phase separately.</p> <pre><code>import requests\nimport time\n\nBASE_URL = \"http://localhost:8000\"\n\ndef process_step_by_step(question: str):\n    \"\"\"Run the council process step by step.\"\"\"\n\n    # Step 1: Create session\n    print(\"Creating session...\")\n    response = requests.post(f\"{BASE_URL}/session\", json={\"question\": question})\n    session_id = response.json()[\"session\"][\"id\"]\n    print(f\"   Session ID: {session_id}\")\n\n    # Step 2: Collect responses\n    print(\"\\nCollecting council responses...\")\n    start = time.time()\n    requests.post(f\"{BASE_URL}/session/{session_id}/responses\")\n    print(f\"   Done in {time.time() - start:.1f}s\")\n\n    # Step 3: Peer reviews\n    print(\"\\nCollecting peer reviews...\")\n    start = time.time()\n    requests.post(f\"{BASE_URL}/session/{session_id}/reviews\")\n    print(f\"   Done in {time.time() - start:.1f}s\")\n\n    # Step 4: Synthesis\n    print(\"\\n\u2728 Synthesizing final answer...\")\n    start = time.time()\n    result = requests.post(f\"{BASE_URL}/session/{session_id}/synthesize\")\n    print(f\"   Done in {time.time() - start:.1f}s\")\n\n    # Return the final answer\n    return result.json()[\"session\"][\"rounds\"][0][\"final_synthesis\"]\n\n\n# Usage\nanswer = process_step_by_step(\"How do I improve my code review process?\")\nprint(\"\\n\" + \"=\" * 60)\nprint(\"FINAL ANSWER:\")\nprint(\"=\" * 60)\nprint(answer)\n</code></pre>"},{"location":"examples/basic-usage/#error-handling","title":"Error Handling","text":"<p>Proper error handling for production use.</p> <pre><code>import requests\nfrom typing import Optional\n\nBASE_URL = \"http://localhost:8000\"\n\nclass CouncilError(Exception):\n    \"\"\"Custom exception for council errors.\"\"\"\n    pass\n\ndef ask_council_safe(question: str) -&gt; Optional[str]:\n    \"\"\"Ask the council with proper error handling.\"\"\"\n    try:\n        # Create session\n        response = requests.post(\n            f\"{BASE_URL}/session\",\n            json={\"question\": question},\n            timeout=10\n        )\n        response.raise_for_status()\n        session_id = response.json()[\"session\"][\"id\"]\n\n        # Run council (with longer timeout)\n        result = requests.post(\n            f\"{BASE_URL}/session/{session_id}/run-all\",\n            timeout=120  # Council process can take a while\n        )\n        result.raise_for_status()\n\n        data = result.json()\n\n        # Check for valid synthesis\n        synthesis = data[\"session\"][\"rounds\"][0].get(\"final_synthesis\")\n        if not synthesis:\n            raise CouncilError(\"No synthesis generated\")\n\n        return synthesis\n\n    except requests.exceptions.Timeout:\n        print(\"Error: Request timed out\")\n        return None\n    except requests.exceptions.ConnectionError:\n        print(\"Error: Cannot connect to API\")\n        return None\n    except requests.exceptions.HTTPError as e:\n        print(f\"Error: HTTP {e.response.status_code}\")\n        return None\n    except CouncilError as e:\n        print(f\"Error: {e}\")\n        return None\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return None\n\n\n# Usage\nanswer = ask_council_safe(\"What is Docker?\")\nif answer:\n    print(answer)\nelse:\n    print(\"Failed to get answer from council\")\n</code></pre>"},{"location":"examples/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Multi-turn Conversations - Continue conversations</li> <li>API Reference - Full API documentation</li> </ul>"},{"location":"examples/multi-turn/","title":"Multi-turn Conversations","text":"<p>Learn how to have extended conversations with the council.</p>"},{"location":"examples/multi-turn/#overview","title":"Overview","text":"<p>LLM Council supports multi-turn conversations where you can ask follow-up questions. The council maintains context from previous rounds, allowing for deeper exploration of topics.</p>"},{"location":"examples/multi-turn/#basic-multi-turn-example","title":"Basic Multi-turn Example","text":"<pre><code>import requests\n\nBASE_URL = \"http://localhost:8000\"\n\nclass CouncilChat:\n    \"\"\"A simple wrapper for multi-turn council conversations.\"\"\"\n\n    def __init__(self):\n        self.session_id = None\n\n    def ask(self, question: str) -&gt; str:\n        \"\"\"Ask a question (creates new session or continues existing).\"\"\"\n        if self.session_id is None:\n            # Create new session\n            response = requests.post(\n                f\"{BASE_URL}/session\",\n                json={\"question\": question}\n            )\n            self.session_id = response.json()[\"session\"][\"id\"]\n        else:\n            # Continue existing session\n            requests.post(\n                f\"{BASE_URL}/session/{self.session_id}/continue\",\n                json={\"question\": question}\n            )\n\n        # Run the council\n        result = requests.post(f\"{BASE_URL}/session/{self.session_id}/run-all\")\n        rounds = result.json()[\"session\"][\"rounds\"]\n\n        # Return the latest synthesis\n        return rounds[-1][\"final_synthesis\"]\n\n    def reset(self):\n        \"\"\"Start a new conversation.\"\"\"\n        self.session_id = None\n\n\n# Usage\nchat = CouncilChat()\n\n# First question\nprint(\"Q1: What is machine learning?\")\nanswer1 = chat.ask(\"What is machine learning?\")\nprint(f\"A1: {answer1}\\n\")\n\n# Follow-up\nprint(\"Q2: What are the main types?\")\nanswer2 = chat.ask(\"What are the main types?\")\nprint(f\"A2: {answer2}\\n\")\n\n# Another follow-up\nprint(\"Q3: Which type is best for image recognition?\")\nanswer3 = chat.ask(\"Which type is best for image recognition?\")\nprint(f\"A3: {answer3}\")\n</code></pre>"},{"location":"examples/multi-turn/#viewing-conversation-history","title":"Viewing Conversation History","text":"<p>Access the full conversation history.</p> <pre><code>import requests\n\nBASE_URL = \"http://localhost:8000\"\n\ndef print_conversation_history(session_id: str):\n    \"\"\"Print the full conversation history.\"\"\"\n    response = requests.get(f\"{BASE_URL}/session/{session_id}\")\n    session = response.json()[\"session\"]\n\n    print(f\"Session: {session['title']}\")\n    print(f\"Rounds: {len(session['rounds'])}\")\n    print(\"=\" * 60)\n\n    for i, round_data in enumerate(session[\"rounds\"], 1):\n        print(f\"\\n\ud83d\udccd Round {i}\")\n        print(f\"   Q: {round_data['question']}\")\n        print(f\"   Status: {round_data['status']}\")\n\n        if round_data.get(\"final_synthesis\"):\n            # Truncate for display\n            synthesis = round_data[\"final_synthesis\"]\n            preview = synthesis[:200] + \"...\" if len(synthesis) &gt; 200 else synthesis\n            print(f\"   A: {preview}\")\n\n\n# Usage\nprint_conversation_history(\"your-session-id\")\n</code></pre>"},{"location":"examples/multi-turn/#context-aware-conversations","title":"Context-Aware Conversations","text":"<p>The council uses previous rounds as context.</p> <pre><code>import requests\n\nBASE_URL = \"http://localhost:8000\"\n\ndef deep_dive_conversation():\n    \"\"\"Example of a deep-dive conversation.\"\"\"\n\n    # Start with a broad question\n    response = requests.post(\n        f\"{BASE_URL}/session\",\n        json={\"question\": \"What are microservices?\"}\n    )\n    session_id = response.json()[\"session\"][\"id\"]\n    requests.post(f\"{BASE_URL}/session/{session_id}/run-all\")\n\n    follow_ups = [\n        \"What are the main benefits?\",\n        \"What are the challenges?\",\n        \"How do I decide if my project needs microservices?\",\n        \"Can you give me a simple example architecture?\"\n    ]\n\n    for question in follow_ups:\n        print(f\"\\n{question}\")\n\n        # Continue the conversation\n        requests.post(\n            f\"{BASE_URL}/session/{session_id}/continue\",\n            json={\"question\": question}\n        )\n\n        result = requests.post(f\"{BASE_URL}/session/{session_id}/run-all\")\n        answer = result.json()[\"session\"][\"rounds\"][-1][\"final_synthesis\"]\n\n        # Print truncated answer\n        preview = answer[:300] + \"...\" if len(answer) &gt; 300 else answer\n        print(f\"{preview}\")\n\n    return session_id\n\n\n# Usage\nsession_id = deep_dive_conversation()\nprint(f\"\\nFull conversation saved in session: {session_id}\")\n</code></pre>"},{"location":"examples/multi-turn/#branching-conversations","title":"Branching Conversations","text":"<p>Start new conversations from any point.</p> <pre><code>import requests\n\nBASE_URL = \"http://localhost:8000\"\n\ndef branch_conversation(original_session_id: str, new_question: str) -&gt; str:\n    \"\"\"\n    Start a new conversation branch based on an existing session.\n\n    Note: This creates a new session - the original is preserved.\n    \"\"\"\n    # Get original session for context\n    response = requests.get(f\"{BASE_URL}/session/{original_session_id}\")\n    original = response.json()[\"session\"]\n\n    # Build context from original\n    context_parts = []\n    for round_data in original[\"rounds\"]:\n        context_parts.append(f\"Q: {round_data['question']}\")\n        if round_data.get(\"final_synthesis\"):\n            context_parts.append(f\"A: {round_data['final_synthesis'][:500]}\")\n\n    context = \"\\n\".join(context_parts)\n\n    # Create new session with context in the question\n    full_question = f\"\"\"Based on this previous conversation:\n---\n{context}\n---\n\nNew question: {new_question}\"\"\"\n\n    response = requests.post(f\"{BASE_URL}/session\", json={\"question\": full_question})\n    new_session_id = response.json()[\"session\"][\"id\"]\n\n    # Run the council\n    result = requests.post(f\"{BASE_URL}/session/{new_session_id}/run-all\")\n\n    return result.json()[\"session\"][\"rounds\"][0][\"final_synthesis\"]\n\n\n# Usage\n# Assuming you have an existing session about Python\n# branch_answer = branch_conversation(\"original-session-id\", \"How does this compare to JavaScript?\")\n</code></pre>"},{"location":"examples/multi-turn/#interactive-cli-chat","title":"Interactive CLI Chat","text":"<p>A complete interactive chat example.</p> <pre><code>import requests\n\nBASE_URL = \"http://localhost:8000\"\n\ndef interactive_chat():\n    \"\"\"Run an interactive chat session with the council.\"\"\"\n    print(\"=\" * 60)\n    print(\"LLM Council Interactive Chat\")\n    print(\"Type 'quit' to exit, 'new' to start fresh\")\n    print(\"=\" * 60)\n\n    session_id = None\n\n    while True:\n        # Get user input\n        try:\n            question = input(\"\\nYou: \").strip()\n        except (KeyboardInterrupt, EOFError):\n            print(\"\\nGoodbye!\")\n            break\n\n        if not question:\n            continue\n\n        if question.lower() == 'quit':\n            print(\"Goodbye!\")\n            break\n\n        if question.lower() == 'new':\n            session_id = None\n            print(\"Starting new conversation...\")\n            continue\n\n        try:\n            if session_id is None:\n                # Create new session\n                response = requests.post(\n                    f\"{BASE_URL}/session\",\n                    json={\"question\": question}\n                )\n                session_id = response.json()[\"session\"][\"id\"]\n            else:\n                # Continue session\n                requests.post(\n                    f\"{BASE_URL}/session/{session_id}/continue\",\n                    json={\"question\": question}\n                )\n\n            print(\"\\nCouncil is deliberating...\")\n\n            # Run council\n            result = requests.post(f\"{BASE_URL}/session/{session_id}/run-all\")\n            answer = result.json()[\"session\"][\"rounds\"][-1][\"final_synthesis\"]\n\n            print(f\"\\nCouncil: {answer}\")\n\n        except Exception as e:\n            print(f\"\\nError: {e}\")\n\n\nif __name__ == \"__main__\":\n    interactive_chat()\n</code></pre>"},{"location":"examples/multi-turn/#best-practices","title":"Best Practices","text":"<p>Tips for Multi-turn Conversations</p> <ol> <li>Be Specific: Follow-up questions work best when they reference previous context</li> <li>Build Incrementally: Start broad, then narrow down</li> <li>Use Pronouns: \"Can you explain that further?\" works because context is maintained</li> <li>Check Status: Always verify the previous round is complete before continuing</li> </ol> <p>Limitations</p> <ul> <li>Each round adds to context length (may hit model limits)</li> <li>Very long conversations may lose early context</li> <li>Consider starting fresh for unrelated topics</li> </ul>"},{"location":"examples/multi-turn/#next-steps","title":"Next Steps","text":"<ul> <li>Basic Usage - Foundational examples</li> <li>API Reference - Full session API</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>Learn how to customize your LLM Council setup.</p>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>All configuration is done through environment variables in the <code>.env</code> file.</p>"},{"location":"getting-started/configuration/#required-variables","title":"Required Variables","text":"Variable Description Example <code>OPENROUTER_API_KEY</code> Your OpenRouter API key <code>sk-or-v1-...</code> <code>MONGODB_URL</code> MongoDB connection string <code>mongodb://localhost:27017</code> <code>MONGODB_DATABASE</code> Database name <code>llm_council</code>"},{"location":"getting-started/configuration/#example-env-file","title":"Example .env File","text":"<pre><code># API Keys\nOPENROUTER_API_KEY=sk-or-v1-your-key-here\n\n# Database\nMONGODB_URL=mongodb://localhost:27017\nMONGODB_DATABASE=llm_council\n\n# Optional: Custom model configuration\n# See config.py for model setup\n</code></pre>"},{"location":"getting-started/configuration/#configuring-council-models","title":"Configuring Council Models","text":"<p>The council models are configured in <code>backend/config.py</code>.</p>"},{"location":"getting-started/configuration/#default-configuration","title":"Default Configuration","text":"<p>LLM Council uses free-tier models from OpenRouter by default:</p> <pre><code>COUNCIL_MODELS = [\n    {\n        \"id\": \"nvidia/nemotron-nano-9b-v2:free\",\n        \"name\": \"NVIDIA Nemotron 9B\",\n        \"provider\": \"openrouter\"\n    },\n    {\n        \"id\": \"nvidia/nemotron-nano-12b-v2-vl:free\",\n        \"name\": \"NVIDIA: Nemotron Nano 12B 2 VL\",\n        \"provider\": \"openrouter\"\n    },\n    {\n        \"id\": \"google/gemma-3-27b-it:free\",\n        \"name\": \"Gemma 3 27B\",\n        \"provider\": \"openrouter\"\n    },\n    {\n        \"id\": \"openai/gpt-oss-20b:free\",\n        \"name\": \"GPT OSS 20B\",\n        \"provider\": \"openrouter\"\n    },\n]\n\n# Chairman model (Grok 4.1 Fast via OpenRouter)\nCHAIRMAN_MODEL = {\n    \"id\": \"x-ai/grok-4.1-fast:free\",\n    \"name\": \"Grok 4.1 Fast\",\n    \"provider\": \"openrouter\"\n}\n</code></pre>"},{"location":"getting-started/configuration/#customizing-models","title":"Customizing Models","text":"<p>You can modify the council composition in <code>backend/config.py</code>:</p> <pre><code># Example: Use paid models for higher quality\nCOUNCIL_MODELS = [\n    {\"id\": \"openai/gpt-4o\", \"name\": \"GPT-4o\", \"provider\": \"openrouter\"},\n    {\"id\": \"anthropic/claude-3.5-sonnet\", \"name\": \"Claude 3.5\", \"provider\": \"openrouter\"},\n    {\"id\": \"google/gemini-pro-1.5\", \"name\": \"Gemini Pro\", \"provider\": \"openrouter\"},\n]\n\nCHAIRMAN_MODEL = {\n    \"id\": \"anthropic/claude-3.5-sonnet\",\n    \"name\": \"Claude 3.5 Sonnet (Chairman)\",\n    \"provider\": \"openrouter\"\n}\n</code></pre>"},{"location":"getting-started/configuration/#available-models","title":"Available Models","text":"<p>LLM Council uses OpenRouter, which provides access to many models including free-tier options.</p>"},{"location":"getting-started/configuration/#free-tier-models-default","title":"Free-Tier Models (Default)","text":"Model ID Notes NVIDIA Nemotron 9B <code>nvidia/nemotron-nano-9b-v2:free</code> Fast, efficient NVIDIA Nemotron 12B VL <code>nvidia/nemotron-nano-12b-v2-vl:free</code> Vision-language capable Gemma 3 27B <code>google/gemma-3-27b-it:free</code> Google's open model GPT OSS 20B <code>openai/gpt-oss-20b:free</code> OpenAI open source Grok 4.1 Fast <code>x-ai/grok-4.1-fast:free</code> Chairman model"},{"location":"getting-started/configuration/#paid-models-higher-quality","title":"Paid Models (Higher Quality)","text":"Model ID Best For GPT-4o <code>openai/gpt-4o</code> General purpose Claude 3.5 Sonnet <code>anthropic/claude-3.5-sonnet</code> Analysis, writing Gemini Pro 1.5 <code>google/gemini-pro-1.5</code> Long context <p>See the full list at OpenRouter Models.</p>"},{"location":"getting-started/configuration/#cors-configuration","title":"CORS Configuration","text":"<p>By default, the API allows requests from:</p> <ul> <li><code>http://localhost:5173</code> (Vite dev server)</li> <li><code>http://localhost:3000</code> (Alternative port)</li> </ul> <p>To add more origins, modify <code>backend/main.py</code>:</p> <pre><code>app.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\n        \"http://localhost:5173\",\n        \"http://localhost:3000\",\n        \"https://your-production-domain.com\",  # Add your domain\n    ],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n</code></pre>"},{"location":"getting-started/configuration/#performance-tuning","title":"Performance Tuning","text":""},{"location":"getting-started/configuration/#parallel-requests","title":"Parallel Requests","text":"<p>By default, all council members are queried in parallel. This is the fastest approach but uses more API credits simultaneously.</p>"},{"location":"getting-started/configuration/#timeout-settings","title":"Timeout Settings","text":"<p>You can adjust timeouts in the OpenRouter client configuration if needed for slower models.</p>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>How It Works - Understand the council process</li> <li>API Reference - Explore all endpoints</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you set up LLM Council on your local machine.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing LLM Council, ensure you have:</p> <ul> <li>Python 3.10+ for the backend</li> <li>Node.js 18+ for the frontend</li> <li>MongoDB running locally or a MongoDB Atlas connection</li> <li>OpenRouter API Key (or other LLM provider keys)</li> </ul>"},{"location":"getting-started/installation/#backend-setup","title":"Backend Setup","text":""},{"location":"getting-started/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/samueldervishii/llm-council.git\ncd llm-council\n</code></pre>"},{"location":"getting-started/installation/#2-set-up-python-environment","title":"2. Set Up Python Environment","text":"<pre><code>cd backend\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre>"},{"location":"getting-started/installation/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#4-configure-environment-variables","title":"4. Configure Environment Variables","text":"<p>Create a <code>.env</code> file in the <code>backend</code> directory:</p> <pre><code>cp .env.example .env\n</code></pre> <p>Edit the <code>.env</code> file with your configuration:</p> <pre><code># OpenRouter API Key\nOPENROUTER_API_KEY=your_openrouter_api_key_here\n\n# MongoDB Connection\nMONGODB_URL=mongodb://localhost:27017\nMONGODB_DATABASE=llm_council\n</code></pre>"},{"location":"getting-started/installation/#5-start-the-backend","title":"5. Start the Backend","text":"<pre><code>python main.py\n</code></pre> <p>The API will be available at <code>http://localhost:8000</code>.</p> <p>Verify Installation</p> <p>Visit <code>http://localhost:8000</code> in your browser. You should see: <code>json     {\"message\": \"LLM Council API\", \"status\": \"running\", \"version\": \"0.0.7\"}</code></p>"},{"location":"getting-started/installation/#frontend-setup","title":"Frontend Setup","text":""},{"location":"getting-started/installation/#1-navigate-to-frontend-directory","title":"1. Navigate to Frontend Directory","text":"<pre><code>cd ../frontend\n</code></pre>"},{"location":"getting-started/installation/#2-install-dependencies","title":"2. Install Dependencies","text":"<pre><code>npm install\n</code></pre>"},{"location":"getting-started/installation/#3-start-the-development-server","title":"3. Start the Development Server","text":"<pre><code>npm run dev\n</code></pre> <p>The frontend will be available at <code>http://localhost:5173</code>.</p>"},{"location":"getting-started/installation/#mongodb-setup","title":"MongoDB Setup","text":""},{"location":"getting-started/installation/#option-1-local-mongodb","title":"Option 1: Local MongoDB","text":"<p>Install MongoDB locally following the official guide.</p> <p>Start MongoDB:</p> <pre><code>mongod\n</code></pre>"},{"location":"getting-started/installation/#option-2-mongodb-atlas-cloud","title":"Option 2: MongoDB Atlas (Cloud)","text":"<ol> <li>Create a free account at MongoDB Atlas</li> <li>Create a new cluster</li> <li>Get your connection string</li> <li>Update <code>MONGODB_URL</code> in your <code>.env</code> file</li> </ol>"},{"location":"getting-started/installation/#verify-everything-works","title":"Verify Everything Works","text":"<ol> <li>Backend: Visit <code>http://localhost:8000/redoc</code> to see the API documentation</li> <li>Frontend: Visit <code>http://localhost:5173</code> to see the chat interface</li> <li>Test a Query: Ask a question in the chat to verify the council is working</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart Guide - Learn how to use LLM Council</li> <li>Configuration - Customize your council setup</li> <li>API Reference - Explore the full API</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>Get up and running with LLM Council in 5 minutes.</p>"},{"location":"getting-started/quickstart/#starting-the-services","title":"Starting the Services","text":"<p>Make sure both backend and frontend are running:</p>"},{"location":"getting-started/quickstart/#backend","title":"\"Backend\"","text":"<pre><code>    cd backend\n    source venv/bin/activate\n    python main.py\n</code></pre>"},{"location":"getting-started/quickstart/#frontend","title":"\"Frontend\"","text":"<pre><code>    cd frontend\n    npm run dev\n</code></pre>"},{"location":"getting-started/quickstart/#using-the-web-interface","title":"Using the Web Interface","text":"<p>The easiest way to use LLM Council is through the web interface.</p>"},{"location":"getting-started/quickstart/#1-open-the-chat","title":"1. Open the Chat","text":"<p>Navigate to <code>http://localhost:5173</code> in your browser.</p>"},{"location":"getting-started/quickstart/#2-ask-a-question","title":"2. Ask a Question","text":"<p>Type your question in the input field and press Enter or click the send button.</p>"},{"location":"getting-started/quickstart/#3-watch-the-council-deliberate","title":"3. Watch the Council Deliberate","text":"<p>The council will:</p> <ol> <li>Query all member LLMs</li> <li>Collect peer reviews</li> <li>Synthesize a final answer</li> </ol>"},{"location":"getting-started/quickstart/#4-continue-the-conversation","title":"4. Continue the Conversation","text":"<p>Ask follow-up questions to dive deeper into the topic.</p>"},{"location":"getting-started/quickstart/#using-the-api","title":"Using the API","text":"<p>You can also interact with LLM Council programmatically.</p>"},{"location":"getting-started/quickstart/#simple-request-python","title":"Simple Request (Python)","text":"<pre><code>import requests\n\nBASE_URL = \"http://localhost:8000\"\n\n# Create a session with your question\nresponse = requests.post(f\"{BASE_URL}/session\", json={\n    \"question\": \"What are the key principles of clean code?\"\n})\nsession = response.json()[\"session\"]\nsession_id = session[\"id\"]\n\nprint(f\"Session created: {session_id}\")\n\n# Run the full council process\nresult = requests.post(f\"{BASE_URL}/session/{session_id}/run-all\")\nfinal = result.json()[\"session\"][\"rounds\"][0]\n\n# Print the results\nprint(\"\\nCouncil Responses:\")\nfor resp in final[\"responses\"]:\n    print(f\"\\n{resp['model_name']}:\")\n    print(resp[\"response\"][:200] + \"...\")\n\nprint(\"\\nFinal Synthesis:\")\nprint(final[\"final_synthesis\"])\n</code></pre>"},{"location":"getting-started/quickstart/#using-curl","title":"Using cURL","text":"<pre><code># Create a session\nSESSION=$(curl -s -X POST http://localhost:8000/session \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"question\": \"What is the best programming language for beginners?\"}' \\\n  | jq -r '.session.id')\n\necho \"Session ID: $SESSION\"\n\n# Run the full council process\ncurl -X POST \"http://localhost:8000/session/$SESSION/run-all\" | jq '.session.rounds[0].final_synthesis'\n</code></pre>"},{"location":"getting-started/quickstart/#step-by-step-control","title":"Step-by-Step Control","text":"<p>For more control, you can run each step individually:</p> <pre><code>import requests\n\nBASE_URL = \"http://localhost:8000\"\nsession_id = \"your-session-id\"\n\n# Step 1: Get council responses\nrequests.post(f\"{BASE_URL}/session/{session_id}/responses\")\n\n# Step 2: Get peer reviews\nrequests.post(f\"{BASE_URL}/session/{session_id}/reviews\")\n\n# Step 3: Get final synthesis\nresult = requests.post(f\"{BASE_URL}/session/{session_id}/synthesize\")\n\nprint(result.json()[\"session\"][\"rounds\"][0][\"final_synthesis\"])\n</code></pre>"},{"location":"getting-started/quickstart/#multi-turn-conversations","title":"Multi-turn Conversations","text":"<p>Continue the conversation with follow-up questions:</p> <pre><code># After the first round is complete...\n\n# Add a follow-up question\nrequests.post(f\"{BASE_URL}/session/{session_id}/continue\", json={\n    \"question\": \"Can you give me specific examples?\"\n})\n\n# Run the council again\nresult = requests.post(f\"{BASE_URL}/session/{session_id}/run-all\")\n\n# The council now has context from the previous round\nprint(result.json()[\"session\"][\"rounds\"][1][\"final_synthesis\"])\n</code></pre>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>Configuration - Customize your council models</li> <li>How It Works - Understand the council process</li> <li>API Reference - Explore all endpoints</li> </ul>"}]}